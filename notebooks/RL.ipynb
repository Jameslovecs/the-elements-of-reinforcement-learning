{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Symbol &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Meaning|\n",
    "|-------------|:-------------|\n",
    "|$t$ | Time step. |\n",
    "|$S$ | State space. |\n",
    "|$s_t$ | State at time $t$. |\n",
    "|$s_{t+1}$ | State at time $t+1$. |\n",
    "|$s'$ | State at the next time step. Equavelent to $s_{t+1}$|\n",
    "|$A$ | Action space. |\n",
    "|$a_t$ | Action at time $t$. |\n",
    "|$a_{t+1}$ | Action at time $t+1$. |\n",
    "|$a'$ | Action at the next time step. Equavelent to $a_{t+1}$|\n",
    "|$\\pi(a\\mid s)$ | Policy, which is the probability of taking action $a$ at state $s$. |\n",
    "|$\\hat \\pi(a\\mid s;\\mathbf{\\theta})$ | Policy approximation function parameterized by $\\mathbf{\\theta}$. |\n",
    "|$p(s_{t+1}\\mid s_t, a_t)$ | State transition probability, which is the probability of translating from state $s_t$ to $s_{t+1}$ by taking action $a_t$. |\n",
    "|$trajectory_{\\pi,p}=[(s_t,a_t),(s_{t+1},a_{t+1}),...,(s_{t+\\infty},a_{t+\\infty})]$ | Trajectory, which is a sequence of (state, action) pair from time $t$ to $\\infty$, following policy $\\pi$ and state transition probability $p$.|\n",
    "|$r(s, a)$ | Reward function, which gives an immediate reward for taking the action $a$ at state $s$. |\n",
    "|$r_{t+1}=r(s_t, a_t)$ | Reward, by taking action $a_t$ at state $s_t$. |\n",
    "|$\\gamma$ | Discount factor. $\\gamma \\in (0,1]$. |\n",
    "|$\\eta$ | Learning rate. $\\eta \\in (0,1]$. |\n",
    "|$\\lambda$ | Eligibility factor. $\\lambda \\in (0,1]$. |\n",
    "|$g_t=\\sum_{k=0}^{\\infty}\\gamma^k r_{t+1+k}$ | Return, which is the discounted, accumulated reward from time $t$ to $\\infty$.|\n",
    "|$v_\\pi(s)=$ <br> $E[g_t\\mid s_t=s]=$ <br> $\\sum_{a}\\pi(a\\mid s)[r(s,a)+\\gamma\\sum_{s'}p(s' \\mid s, a)v_\\pi(s')]$  |State value, which is the return expectation (expectation over all trajectories) for following policy $\\pi$ from state $s$. |\n",
    "|$v_\\ast(s)=$ <br> $\\text{max}_a[r(s,a)+\\gamma\\sum_{s'}p(s' \\mid s, a)v_\\ast(s')]$ |Optimal state value. |\n",
    "|$\\hat v(s;\\mathbf{w})$ | State value approximation function parameterized by a weight vector $\\mathbf{w}$.|\n",
    "|$\\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w})=\\frac{\\partial \\hat v(s;\\mathbf{w})}{\\partial \\mathbf{w}}$ | Partial derivative of $\\hat v(s;\\mathbf{w})$ with respect to $\\mathbf{w}$|\n",
    "|$q_\\pi(s,a)=$ <br> $E[g_t\\mid s_t=s, a_t=a]=$ <br> $r(s,a)+\\gamma\\sum_{s'}[p(s' \\mid s, a) \\sum_{a'}\\pi(a'\\mid s')q_\\pi(s',a')]$ | State-action value, which is the return expectation (expectation over all trajectories) for selecting action $a$ in state $s$ and then following policy $\\pi$.|\n",
    "|$q_\\ast(s,a)=$ <br> $r(s,a)+\\gamma\\sum_{s'}[p(s' \\mid s, a)\\text{max}_{a'}\\ \\pi(a'\\mid s')q_\\ast(s',a')]$ | Optimal state-action value. |\n",
    "|$\\pi_\\ast$ | Optimal policy. |\n",
    "|5-tuple $(S, A, P, R, \\gamma)$ | Markov Decision Process (MDP) <br> $S$: state space; <br> $A$: action space; <br> $P$: state transition probability matrix, <br> $P=p(s' \\mid s, a),\\forall s,s',a$; <br> $R$: reward matrix,<br> $R=r(s,a),\\forall s, a$; <br> $\\gamma$: discount factor. |\n",
    "|$\\delta_{TD}(s, s',a,\\gamma)=[r(s,a)+\\gamma v(s')-v(s)]$ | Temporal difference (TD) error |\n",
    "|$\\hat\\delta_{TD}(s, s',a,\\gamma, \\mathbf{w})=[r(s,a)+\\gamma \\hat v(s',\\mathbf{w})-\\hat v(s;\\mathbf{w})]$ | TD error approximation |\n",
    "|$\\delta_{BM}(s,\\gamma)=\\sum_{a}\\pi(a\\mid s)\\sum_{s', a}p(s' \\mid s, a)[\\delta_{TD}(s, s',a,\\gamma)]$ | Bellman error, which is the expectation of the TD error.|\n",
    "|$v(s)\\leftarrow v(s)+\\eta\\ \\delta_{TD}(s, s',a,\\gamma)$ | $TD(0)$ learning.|\n",
    "|$\\mathbf{w}\\leftarrow \\mathbf{w}+\\eta\\ \\hat\\delta_{TD}(s, s',a,\\gamma, \\mathbf{w})\\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w})$ |$TD(0)$ learning using state value approximation function, parameterized by $\\mathbf{w}$. |\n",
    "|Initialize $e$ (eligibility) to be 1. <br> $v(s)\\leftarrow v(s)+\\eta\\ \\delta_{TD}(s, s',a,\\gamma)e$ <br> $e\\leftarrow\\gamma\\lambda e$| $TD(\\lambda)$ learning|\n",
    "|Initialize $e$ (eligibility) to be $\\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w})$. <br> $v(s)\\leftarrow v(s)+\\eta\\ \\hat\\delta_{TD}(s, s',a,\\gamma, \\mathbf{w})e$ <br> $e\\leftarrow \\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w}) + \\gamma\\lambda e$| $TD(\\lambda)$ learning using state value approximation function, parameterized by $\\mathbf{w}$.|\n",
    "|$The\\ end.$ | |\n",
    "\n",
    "* Sup (\"supremum\") means, basically, the largest.\n",
    "* Lipschitz function, a function f such that\n",
    "\\begin{equation}\n",
    " |f(x)-f(y)|<=C|x-y| \n",
    "\\end{equation}\n",
    "for all x and y, where C is a constant independent of x and y, is called a Lipschitz function. For example, any function with a bounded first derivative must be Lipschitz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL agent \n",
    "A RL agent executes a sequence of actions and observe states and rewards, with major components of value function, policy and model.\n",
    "\n",
    "### RL problem\n",
    "A RL problem may be formulated as a prediction, control or\n",
    "planning problem.\n",
    "* prediction\n",
    "    * The prediction problem, or policy evaluation, is to compute the state or state-action value function for a policy.\n",
    "        * TD learning algorithms are fundamental for evaluating/predicting value functions.\n",
    "            * SARSA (Sutton and Barto, 2017) \n",
    "            * Q-learning (Watkins and Dayan, 1992)\n",
    "* control\n",
    "    * Control algorithms find optimal policies.\n",
    "* planning\n",
    "\n",
    "### Large scale problem\n",
    "Turning the infeasible dynamic programming methods into practical algorithms so that they can be applied to large-scale problems.\n",
    "* use samples to compactly represent the dynamics of the control problem.\n",
    "    * it allows one to deal with learning scenarios when the dynamics is unknown. \n",
    "    * even if the dynamics is available, exact reasoning that uses it might be intractable on its own.\n",
    "* use powerful function approximation methods to compactly represent value functions.\n",
    "\n",
    "### RL algorithms\n",
    "* value VS policy\n",
    "    * value based: \n",
    "        * state value\n",
    "        * state-action value\n",
    "    * policy based\n",
    "* model-free VS ,model-based:\n",
    "    * model-free: the model (state transition function) is not known or learned from experience.\n",
    "    * When the system model is available, we use dynamic programming, which transforms the problem of finding a good controller into the problem of finding a good value function.\n",
    "    * When there is no model, we resort to RL methods. RL methods also work when the model is available.\n",
    "* policy:\n",
    "    * The notion of on-policy and off-policy can be understood as same-policy and different-policy.\n",
    "    * on-policy: evaluate or improve the behavioural policy. \n",
    "        * SARSA evaluates the policy based on samples from the same policy, then refines the policy greedily with respect to action values.\n",
    "    * off-policy: learns an optimal value function/policy, maybe following an unrelated behavioural policy\n",
    "        * the policy Q-learning obtains is usually different from the policy that generates the samples.\n",
    "* function approximation\n",
    "    * with:\n",
    "    * without:\n",
    "* backups\n",
    "    * sample backups (TD and Monte Carlo) \n",
    "        * one-step return (TD(0) and dynamic programming) \n",
    "        * multi-step return (TD(Î»), Monte Carlo, and exhaustive search)\n",
    "    * full backups (dynamic programming and exhaustive search)\n",
    "* online learning VS batch learning\n",
    "    * online learning: training algorithms are executed on data acquired in sequence.\n",
    "    * batch learning: models are trained on the entire data set.\n",
    "\n",
    "### When combining off-policy, function approximation, and bootstrapping, we face instability and divergence.\n",
    "\n",
    "* function approximation for scalability and generalization, \n",
    "    * Linear function approximation: Gradient-TD (Sutton et al., 2009a;b; Mahmood et al., 2014), Emphatic-TD (Sutton et al., 2016) and Du et al. (2017)\n",
    "    * Non-linear function approximation: Deep Q-Network (Mnih et al., 2015) and AlphaGo (Silver et al., 2016a)\n",
    "* bootstrapping for computational and data efficiency,\n",
    "    * bootstrapping, an estimate of state or action value is updated from subsequent estimates.\n",
    "* off-policy learning for freeing behaviour policy from target policy.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asynchronous advantage actor-critic (A3C)\n",
    "* learns both a policy and a state-value function\n",
    "    * the value function is used for bootstrapping, i.e., updating a state from subsequent estimates, to reduce variance and accelerate learning.\n",
    "* parallel actors employ different exploration policies to stabilize training, so that experience replay is not utilized.\n",
    "* Experience replay: build data-set from agentâs experience\n",
    "* Critic estimates value of current policy by DQN\n",
    "* Actor updates policy in direction that improves Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

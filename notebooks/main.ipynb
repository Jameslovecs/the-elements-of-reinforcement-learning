{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaitao Yang\n",
    "#### Deep Learning Scientist @ eBay, Amsterdam\n",
    "#### Deep Learning Lecturer @ [DL-APPLIED](www.dlapplied.com)\n",
    "#### [Email](ykaitao@hotmail.com), [LinkedIn](https://www.linkedin.com/in/kaitaoyang/) \n",
    "#### Deep Learning Training, May 26, 27 (Saturday and Sunday), 2018, Utrecht, [Sign in ](https://dlapplied.com/deep-learning-training/) now to get <span style=\"color:blue\"> 85% OFF</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from stock_game import state_set, action_set, num_states, num_actions, \\\n",
    "    initial_state, take_action_by_human, environment_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State set: ['AMZN', 'GOOG', 'FB']\n",
      "Action set: ['b', 'h', 's']\n"
     ]
    }
   ],
   "source": [
    "print('State set:', state_set) \n",
    "print('Action set:', action_set) # 'b', 'h', and 's' means buy, hold, sell, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def print_and_log_string(file_handle, string):\n",
    "    print(string)\n",
    "    file_handle.write(string)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type in anything in the box.\n",
      "Episode 5 starts, Your initial state: FB.\n",
      "\n",
      "s\n",
      "Time: 1; \n",
      "Action: s; \n",
      "New state: AMZN; \n",
      "Reward: 15 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 2; \n",
      "Action: b; \n",
      "New state: GOOG; \n",
      "Reward: 27 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 3; \n",
      "Action: b; \n",
      "New state: GOOG; \n",
      "Reward: 13 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 4; \n",
      "Action: b; \n",
      "New state: AMZN; \n",
      "Reward: -19 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 5; \n",
      "Action: b; \n",
      "New state: FB; \n",
      "Reward: -5 k€.\n",
      "=======\n",
      "\n",
      "s\n",
      "Time: 6; \n",
      "Action: s; \n",
      "New state: AMZN; \n",
      "Reward: 11 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 7; \n",
      "Action: b; \n",
      "New state: GOOG; \n",
      "Reward: 21 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 8; \n",
      "Action: b; \n",
      "New state: GOOG; \n",
      "Reward: 14 k€.\n",
      "=======\n",
      "\n",
      "b\n",
      "Time: 9; \n",
      "Action: b; \n",
      "New state: GOOG; \n",
      "Reward: 22 k€.\n",
      "=======\n",
      "\n",
      "End of episode, your total gain is 101 k€ in this episode.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# play by human\n",
    "print('Please type in anything in the box.')\n",
    "\n",
    "is_first_episode = (episode == 0)\n",
    "if is_first_episode:\n",
    "    log_file_handle = open('playing_history_human.txt', 'w')\n",
    "else:\n",
    "    log_file_handle = open('playing_history_human.txt', 'a')\n",
    "    \n",
    "\n",
    "s = initial_state()\n",
    "T = 10 # total steps in each episode\n",
    "t = 1 # time step\n",
    "G = 0 # cumulative gain\n",
    "\n",
    "string_print_log = 'Episode %d starts, Your initial state: %s.\\n' % (episode,s)\n",
    "print_and_log_string(file_handle=log_file_handle, string=string_print_log)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    a = take_action_by_human()\n",
    "    \n",
    "    s_, r_ = environment_response(s, a)\n",
    "    string_print_log = 'Time: %d; \\nAction: %s; \\nNew state: %s; \\nReward: %d k€.\\n=======\\n' % (t, a, s_, r_)\n",
    "    print_and_log_string(file_handle=log_file_handle, string=string_print_log)\n",
    "    \n",
    "    G += r_ # add the reward to cumulative gain\n",
    "    t += 1 # increase time step by 1\n",
    "    s = s_ # replace old state with new state\n",
    "    \n",
    "    end_of_episode = (t==T)\n",
    "    if end_of_episode:\n",
    "        string_print_log = 'End of episode, your total gain is %d k€ in this episode.\\n========================\\n' % G\n",
    "        print_and_log_string(file_handle=log_file_handle, string=string_print_log)\n",
    "        break\n",
    "\n",
    "episode += 1 # episode increase by 1\n",
    "log_file_handle.close() # close log file handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-action value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2i = {state:index for index, state in enumerate(state_set)} # state to index\n",
    "a2i = {action:index for index, action in enumerate(action_set)} # action to index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\eta [R_{t+1}+\\gamma \\underset{a}{\\mathrm{max}}Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00018776973291799315\n",
      "1000 12.592684147341416\n",
      "2000 1.7108837556130307\n",
      "3000 0.7001453469170388\n",
      "4000 0.11881937104012837\n",
      "5000 0.10920629860220286\n",
      "6000 0.07803189747068302\n",
      "7000 0.03889490187353489\n",
      "8000 0.06728121582035113\n",
      "9000 0.12257936929766801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>h</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>7.350263</td>\n",
       "      <td>0.791815</td>\n",
       "      <td>2.752873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>9.386443</td>\n",
       "      <td>0.810907</td>\n",
       "      <td>7.110330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>7.082428</td>\n",
       "      <td>0.824389</td>\n",
       "      <td>8.315850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             b         h         s\n",
       "AMZN  7.350263  0.791815  2.752873\n",
       "GOOG  9.386443  0.810907  7.110330\n",
       "FB    7.082428  0.824389  8.315850"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 0*np.ones(shape=(num_states, num_actions))\n",
    "\n",
    "eta = 0.001 # learning rate\n",
    "gamma = 0.1 # discount factor                    \n",
    "K = 10000 # number of iterations\n",
    "\n",
    "def target(reward, state_new, gamma):\n",
    "        \n",
    "    return reward + gamma*np.max([Q[s2i[state_new], a2i[a]] for a in action_set]) \n",
    "\n",
    "def update_Q():\n",
    "    for s in state_set:\n",
    "        for a in action_set:\n",
    "            s_, r_ = environment_response(s, a) # next state, and reward\n",
    "                        \n",
    "            delta = target(reward=r_, state_new=s_, gamma=gamma) - Q[s2i[s], a2i[a]]\n",
    "            Q[s2i[s], a2i[a]] += eta*delta  \n",
    "            \n",
    "            \n",
    "def q_learning():\n",
    "    Q_old = np.copy(Q)   \n",
    "    for k in range(K):\n",
    "\n",
    "        update_Q()\n",
    "\n",
    "        if k%1000==0:\n",
    "            print(k, mean_squared_error(Q_old, Q))\n",
    "            Q_old = np.copy(Q)  \n",
    "\n",
    "            \n",
    "q_learning()\n",
    "Q_df = pd.DataFrame(data=Q, index=state_set, columns=action_set)\n",
    "Q_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.006819244192929303\n",
      "1000 8.012752591141865\n",
      "2000 1.3479546129496613\n",
      "3000 1.3042756653840073\n",
      "4000 2.0474430557792114\n",
      "5000 0.3515790986806268\n",
      "6000 0.2502489918968549\n",
      "7000 0.2821372429632747\n",
      "8000 0.23469269878046287\n",
      "9000 0.22528130722269338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>h</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>6.639964</td>\n",
       "      <td>2.703175</td>\n",
       "      <td>4.341922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>9.313252</td>\n",
       "      <td>2.728952</td>\n",
       "      <td>7.538979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>6.952583</td>\n",
       "      <td>4.312501</td>\n",
       "      <td>9.168975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             b         h         s\n",
       "AMZN  6.639964  2.703175  4.341922\n",
       "GOOG  9.313252  2.728952  7.538979\n",
       "FB    6.952583  4.312501  9.168975"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_sarsa = 10*np.ones(shape=(num_states, num_actions))\n",
    "\n",
    "eta = 0.01 # learning rate\n",
    "gamma = 0.1 # discount factor \n",
    "K = 10000 # number of iterations\n",
    "epsilon = 0.1 # greedy factor            \n",
    "\n",
    "def epsilon_greedy(array, epsilon=0.1):\n",
    "    \n",
    "    a_greedy = action_set[np.argmax(array)]\n",
    "    a_random = np.random.choice(action_set, size=None)\n",
    "    \n",
    "    return np.random.choice([a_greedy, a_random], size=None, p=[1-epsilon, epsilon])\n",
    "\n",
    "def sarsa_learning():\n",
    "    \n",
    "    s = initial_state()\n",
    "    Q_sarsa_old = np.copy(Q_sarsa) \n",
    "    \n",
    "    for k in range(K):\n",
    "        a = epsilon_greedy(array=Q_sarsa[s2i[s],:], epsilon=epsilon)\n",
    "        s_, r_ = environment_response(s, a) # next state, and reward\n",
    "        a_ = epsilon_greedy(array=Q_sarsa[s2i[s_],:], epsilon=epsilon)\n",
    "\n",
    "        delta = [r_ + gamma*Q_sarsa[s2i[s_], a2i[a_]]] - Q_sarsa[s2i[s], a2i[a]]\n",
    "        Q_sarsa[s2i[s], a2i[a]] += eta*delta \n",
    "        s, a = s_, a_\n",
    "\n",
    "        if k%1000==0:\n",
    "            print(k, mean_squared_error(Q_sarsa_old, Q_sarsa))\n",
    "            Q_sarsa_old = np.copy(Q_sarsa) \n",
    "\n",
    "sarsa_learning()   \n",
    "Q_df = pd.DataFrame(data=Q_sarsa, index=state_set, columns=action_set)\n",
    "Q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 189"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Symbol &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Meaning|\n",
    "|-------------|:-------------|\n",
    "|$t$ | Time step. |\n",
    "|$\\mathcal{S}$ | State space. |\n",
    "|$S_t$ | State at time $t$ as a random variable. |\n",
    "|$s_t$ | State at time $t$ as a sample. |\n",
    "|$s_{t+1}$ | State at time $t+1$. |\n",
    "|$s'$ | State at the next time step. Equavelent to $s_{t+1}$|\n",
    "|$\\mathcal{A}$ | Action space. |\n",
    "|$A_t$ | Action at time $t$ as a random variable. |\n",
    "|$a_t$ | Action at time $t$ as a sample. |\n",
    "|$a_{t+1}$ | Action at time $t+1$. |\n",
    "|$a'$ | Action at the next time step. Equavelent to $a_{t+1}$|\n",
    "|$Pr(.)$ | Probatility. |\n",
    "|$\\pi(a\\mid s)=Pr(A_t=a\\mid S_t=s)$ | Policy, which is the probability of taking action $a$ at state $s$. |\n",
    "|$\\hat \\pi(a\\mid s;\\mathbf{\\theta})$ | Policy approximation function parameterized by $\\mathbf{\\theta}$. |\n",
    "|$p(s_{t+1}\\mid s_t, a_t)=Pr(S_{t+1}=s_{t+1}\\mid S_t=s_t,A_t=a_t)$ | State transition probability, which is the probability of translating from state $s_t$ to $s_{t+1}$ by taking action $a_t$. |\n",
    "|$trajectory_{\\pi,p}=[(a_t, r_{t+1}, s_{t+1}),(a_{t+1}, r_{t+2}, s_{t+2}),...,(a_{\\infty-1}, r_{\\infty}, s_{\\infty})]$ | Trajectory, which is a sequence of (action, new reward, new state) tuples from time $t$ to $\\infty$, following policy $\\pi$ and state transition probability $p$.|\n",
    "|$\\mathcal{R}$ | Reward space. |\n",
    "|$R$ | Reward function. |\n",
    "|$R_{t+1}=R(S_t=s_t,A_t=a_t)$ | Reward at time $t+1$ as a random variable, which is an immediate reward for taking the action $a_t$ at state $s_t$. |\n",
    "|$r_{t+1}$ | A sample of $R_{t+1}$.|\n",
    "|$r'$ | A sample of $R_{t+1}$. Equavelent to $r_{t+1}$|\n",
    "|$\\gamma$ | Discount factor. $\\gamma \\in (0,1]$. |\n",
    "|$\\eta$ | Learning rate. $\\eta \\in (0,1]$. |\n",
    "|$\\lambda$ | Eligibility factor. $\\lambda \\in [0,1]$. |\n",
    "|$G_t=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+1+k}=$ <br> $R_{t+1}+\\gamma G_{t+1}$ | Return, which is the discounted, accumulated reward from time $t$ to $\\infty$.|\n",
    "|$g_{t}$ | A sample of $G_{t}$.|\n",
    "|$v_\\pi(s)=$ <br> $E_\\pi[G_{t}\\mid S_t=s]=$ <br> $E_\\pi[R_{t+1}+\\gamma G_{t+1}\\mid S_t=s]=$ <br> $\\sum_{a}\\pi(a\\mid s)\\sum_{s',r'}p(s',r' \\mid s, a)[r'+\\gamma v_\\pi(s')]$  |State value, which is the return expectation (expectation over all trajectories) for following policy $\\pi$ from state $s$. |\n",
    "|$v_\\ast(s)=$ <br> $\\text{max}_a\\sum_{s',r'}p(s',r' \\mid s, a)[r'+\\gamma v_\\ast(s')]$ |Optimal state value. |\n",
    "|$\\hat v(s;\\mathbf{w})$ | State value approximation function parameterized by a weight vector $\\mathbf{w}$.|\n",
    "|$\\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w})=\\frac{\\partial \\hat v(s;\\mathbf{w})}{\\partial \\mathbf{w}}$ | Partial derivative of $\\hat v(s;\\mathbf{w})$ with respect to $\\mathbf{w}$|\n",
    "|$q_\\pi(s,a)=$ <br> $E_\\pi[G_t\\mid s_t=s, a_t=a]=$ <br> $\\sum_{s',r'}p(s',r' \\mid s, a)[r'+\\gamma \\sum_{a'}\\pi(a'\\mid s')q_\\pi(s',a')]$ | State-action value, which is the return expectation (expectation over all trajectories) for selecting action $a$ in state $s$ and then following policy $\\pi$.|\n",
    "|$q_\\ast(s,a)=$ <br> $\\sum_{s',r'}p(s',r' \\mid s, a)[r'+\\gamma \\text{max}_{a'}q_\\ast(s',a')]$ | Optimal state-action value. |\n",
    "|$\\pi_\\ast$ | Optimal policy. |\n",
    "|5-tuple $(S, A, P, R, \\gamma)$ | Markov Decision Process (MDP) <br> $S$: state space; <br> $A$: action space; <br> $P$: state transition probability matrix, <br> $P=p(s' \\mid s, a),\\forall s,s',a$; <br> $R$: reward function;<br> $\\gamma$: discount factor. |\n",
    "|$\\delta_{TD}(s, s',r',\\gamma)=[r'+\\gamma v(s')-v(s)]$ | Temporal difference (TD) error |\n",
    "|$\\hat\\delta_{TD}(s, s',r',\\gamma, \\mathbf{w})=[r'+\\gamma \\hat v(s';\\mathbf{w})-\\hat v(s;\\mathbf{w})]$ | TD error approximation |\n",
    "|$\\delta_{BM}(s,\\gamma)=\\sum_{a}\\pi(a\\mid s)\\sum_{s', r'}p(s',r' \\mid s, a)[\\delta_{TD}(s, s',r',\\gamma)]$ | Bellman error, which is the expectation of the TD error.|\n",
    "|$v(s)\\leftarrow v(s)+\\eta\\ \\delta_{TD}(s, s',r',\\gamma)$ | $TD(0)$ learning.|\n",
    "|$\\mathbf{w}\\leftarrow \\mathbf{w}+\\eta\\ \\hat\\delta_{TD}(s, s',r',\\gamma, \\mathbf{w})\\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w})$ |$TD(0)$ learning using state value approximation function, parameterized by $\\mathbf{w}$. |\n",
    "|Initialize $e$ (eligibility) to be 1. <br> $v(s)\\leftarrow v(s)+\\eta\\ \\delta_{TD}(s, s',r',\\gamma)e$ <br> $e\\leftarrow\\gamma\\lambda e$| $TD(\\lambda)$ learning|\n",
    "|Initialize $e$ (eligibility) to be $\\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w})$. <br> $v(s)\\leftarrow v(s)+\\eta\\ \\hat\\delta_{TD}(s, s',r',\\gamma, \\mathbf{w})e$ <br> $e\\leftarrow \\nabla_\\mathbf{w}\\hat v(s;\\mathbf{w}) + \\gamma\\lambda e$| $TD(\\lambda)$ learning using state value approximation function, parameterized by $\\mathbf{w}$.|\n",
    "|$The\\ end.$ | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
